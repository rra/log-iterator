#!/usr/bin/perl
#
# webkdc-user-agent -- Parse WebAuth WebKDC logs and generate User-Agent stats
#
# Correlate WebAuth WebKDC logs and Apache access logs to determine the user
# agents of authenticated users.  From that, generate statistics showing how
# many unique users use each user agent.
#
# Written by Russ Allbery <rra@stanford.edu>
# Copyright 2013
#     The Board of Trustees of the Leland Stanford Junior University

use 5.010;
use strict;
use warnings;

use File::Find qw(find);
use Getopt::Long qw(GetOptions);
use HTTP::BrowserDetect;
use List::Util qw(max reduce);
use List::MoreUtils qw(apply mesh);
use Log::Stream::File;
use Log::Stream::File::Gzip;
use Log::Stream::Merge;
use Log::Stream::Merge::Rewindable;
use Log::Stream::Parse::Apache::Combined;
use Log::Stream::Parse::WebKDC;
use Log::Stream::Transform;
use Memoize;
use Readonly;
use Text::CSV;

# Whether we care about the browser version.  Default to yes.
my $WANT_VERSION = 1;

# Regex matching URLs that may be used for WebAuth logins.  This could be
# site-specific if one doesn't use the default recommended URL, but currently
# isn't configurable without editing the script.
Readonly::Scalar my $LOGIN_URL_REGEX => qr{ /login/? }xms;

# How many seconds after the error log entry to look for a corresponding
# access log entry.  This bound will also bound our memory consumption.
Readonly::Scalar my $TIMEOUT => 2;

# Our log merge routine.
#
# Find an entry in the error log stream that represents a successful login.
# Then, try to find a corresponding entry in the access log with the same
# timestamp or later, from the same client IP address, that matches the login
# URL and was successful.  If we find one, return the relevant data as our
# output element from our stream.  Otherwise, ignore that entry and try again.
#
# $error  - Parsed Apache error log stream
# $access - Parsed Apache access log stream
#
# Returns: A hash containing the timestamp, user, and user agent (user_agent)
sub merge_user_agent {
    my ($error_stream, $access_stream) = @_;

    # Search through the error log for something of interest.
  ERROR:
    while (my $entry = $error_stream->get) {
        next ERROR if !$entry->{event} || $entry->{event} ne 'requestToken';
        next ERROR if !defined($entry->{lec}) || $entry->{lec} != 0;
        next ERROR if !defined $entry->{user};
        next ERROR if !defined $entry->{clientIp};

        # We found a login.  Stash some useful data.
        my $client    = $entry->{clientIp};
        my $timestamp = $entry->{timestamp};

        # The access log entry will always be after the error log entry, so we
        # can discard all access log entries with an older timestamp.
        while (my $access = $access_stream->head) {
            last if $access->{timestamp} >= $timestamp;
            $access_stream->get;
        }

        # Now, search for an access log entry that matches.
        $access_stream->bookmark;
      ACCESS:
        while (my $access = $access_stream->get) {
            last ACCESS if $access->{timestamp} > $timestamp + $TIMEOUT;
            next ACCESS if $access->{client} ne $client;
            next ACCESS if !defined $access->{user_agent};
            next ACCESS if !defined $access->{base_query};
            next ACCESS
              if $access->{base_query} !~ m{ \A $LOGIN_URL_REGEX \z }xmso;

            # We found a match.  Create our output token.
            my $result = {
                timestamp  => $entry->{timestamp},
                user       => $entry->{user},
                user_agent => $access->{user_agent},
            };

            # Push back the access log data except the entry we used.
            my @saved = $access_stream->saved;
            pop @saved;
            $access_stream->discard;
            $access_stream->prepend(@saved);

            # Return our event.
            return $result;
        }

        # We failed to find a match.  Discard this error log entry.
        $access_stream->rewind;
    }

    # Fell off the end of the error log stream.  We're done.
    return;
}

# Build a stream of authenticated user agents from input CSV data (that we
# probably wrote out previously).
#
# $csv_file - Input file of CSV data
#
# Returns: Stream of authenticated user agents
sub build_csv_stream {
    my ($csv_file) = @_;

    # Create a CSV parser.
    my $csv = Text::CSV->new;

    # Build a stream from the input file.
    my $csv_stream = Log::Stream::File->new({ files => $csv_file });

    # The first line will be the field names.
    if (!$csv->parse($csv_stream->get)) {
        die "First line of $csv_file is invalid\n";
    }
    my @fields = $csv->fields;

    # Now, create a parser for that stream.
    my $parser = sub {
        my ($line) = @_;
        if (!$csv->parse($line)) {
            die "Cannot parse line: $line\n";
        }
        my @values = $csv->fields;
        if (scalar(@values) != scalar(@fields)) {
            die "Incorrect number of fields on line: $line\n";
        }
        return { mesh(@fields, @values) };
    };

    # Wrap the stream in that parser and return it.
    return Log::Stream::Transform->new($parser, $csv_stream);
}

# Build our stream of authenticated user agents.  Takes two sets of files: a
# reference to an array of error logs and a reference to an array of access
# logs.  These must be from the same machine so they will correlate properly
# and won't move around in time.
#
# Decide whether the log files are compressed based on the first element of
# each array.  We can improve this later with more brains in the
# Log::Stream::File classes.
#
# $error_files  - Reference to array of Apache error logs
# $access_files - Reference to array of Apache access logs
#
# Returns: Stream of authenticated user agents
sub build_log_stream {
    my ($error_files, $access_files) = @_;

    # Determine what file stream class to use.
    my $is_gzip = $error_files->[0] =~ m{ [.]gz \z }xms;
    my $class = $is_gzip ? 'Log::Stream::File::Gzip' : 'Log::Stream::File';

    # Create the error log stream.  We stick a filter in front of it to ignore
    # any lines that don't contain lec=0 so that we don't go to the work of
    # parsing lines we know we won't care about.  (This plus the same for
    # access logs saves about 40% of the running time.)
    my $error_stream = $class->new({ files => $error_files });
    my $filter = sub { m{ \s lec=0 }xms };
    $error_stream = Log::Stream::Filter->new($filter, $error_stream);
    $error_stream = Log::Stream::Parse::WebKDC->new($error_stream);

    # Create the access log stream.  Similarly here we stick a filter in front
    # to ignore any lines that don't include the login URL.
    my $access_stream = $class->new({ files => $access_files });
    $filter = sub { m{ \s $LOGIN_URL_REGEX }xmso };
    $access_stream = Log::Stream::Filter->new($filter, $access_stream);
    $access_stream = Log::Stream::Parse::Apache::Combined->new($access_stream);

    # Construct the merged log parser.
    return Log::Stream::Merge::Rewindable->new(\&merge_user_agent,
        $error_stream, $access_stream);
}

# Build a stream from archived logs.
#
# This layout is probably specific to Stanford.  Our arguments will be one or
# more directories.  Under those directories (possibly recursively) will be
# logs, which will be named in the format:
#
#     <machine>.<type>.gz
#
# Now, for our purposes, we want to build a single stream of all the error and
# access logs for a particular machine found in our search area, but we do not
# want to mingle streams from different machines.  If we do that, we'll lose
# the synchronization between error log and access log.
#
# Therefore, we find all of the machines for which we have pairs of logs, and
# build individual log streams per machine.  Then, we'll merge all of those
# together, sorted by timestamp, for our actual processing.
#
# @directories - Root directories under which to find logs
#
# Returns: Stream of authenticated user agents
sub build_archive_stream {
    my (@directories) = @_;

    # Find appropriately-named logs in the directories.  We cheat a little
    # here and only find error logs, and then reject any error logs that don't
    # have corresponding access logs.
    my %logs_for;
    my $wanted = sub {
        return if !-f $_;

        # Ensure this is an error log and there is a matching access log.
        my $machine;
        if (m{ \A ([^.]+) [.] error_log [.] gz \z }xms) {
            $machine = $1;
        } else {
            return;
        }
        return if !-f "$machine.access_log.gz";

        # Found a good log pair.  Add it to %logs_for by machine.
        $logs_for{$machine} ||= [];
        push @{ $logs_for{$machine} }, $File::Find::name;
    };
    find($wanted, @directories);

    # Now, we have a hash of machines and a list of error logs for each.
    # Build a stream for each machine.
    my @log_streams;
    for my $logs (values %logs_for) {
        my $access_log_for = sub {
            s{ [.] error_log [.] gz \z }{.access_log.gz}xms;
            return $_;
        };
        my @error_logs = sort @{$logs};
        my @access_logs = apply { $access_log_for->() } @error_logs;
        push @log_streams, build_log_stream(\@error_logs, \@access_logs);
    }

    # Finally, we're going to merge the resulting streams together in a way
    # that sorts them by timestamp.
    my $earliest_timestamp = sub {
        $a->head->{timestamp} < $b->head->{timestamp} ? $a : $b;
    };
    my $merge = sub {
        my (@streams) = grep { defined $_->head } @_;
        return if !@streams;
        return $streams[0]->get if @streams == 1;
        my $best = reduce { $earliest_timestamp->() } @streams;
        return $best->get;
    };
    return Log::Stream::Merge->new($merge, @log_streams);
}

# User agent mapping.  Take a user agent string and converts it into something
# somewhat more sensible and with less useless information, namely:
#
#     <browser>/<major-version> (<device> | <os>)
#
# This will hopefully compress the plethora of different user agents into
# something sensible.  Any component that couldn't be mapped is omitted.  When
# all else fails, returns the original user agent string.
#
# $agent - User-Agent string
#
# Returns: Sensible user agent string as described above
sub simple_agent {
    my ($agent) = @_;
    $agent = HTTP::BrowserDetect->new($agent);

    # Gather information.
    my $browser = $agent->browser_string;
    my $version = $agent->public_version;
    my $device  = $agent->device_name;
    my $os      = $agent->os_string;

    # Assemble the result.
    my $result = q{};
    if ($browser) {
        $result .= $browser;
        if ($WANT_VERSION && defined $version) {
            $result .= q{/} . $version;
        }
    } else {
        $result .= 'UNKNOWN';
    }
    if ($device) {
        $result .= " ($device)";
    } elsif ($os) {
        $result .= " ($os)";
    }
    return $result;
}

# Parse command-line options.
my ($archive, $csv, $input);
Getopt::Long::Configure('bundling', 'no_ignore_case');
GetOptions(
    'a|archive'        => \$archive,
    'browser-version!' => \$WANT_VERSION,
    'c|csv'            => \$csv,
    'i|input=s'        => \$input,
) or exit 1;
if ($archive && defined $input) {
    die "Archive flag (-a) does not make sense with CSV input (-i)\n";
}

# If given an input file, read it as CSV data.
my $stream;
if (defined $input) {
    if (@ARGV) {
        die "Usage: webkdc-user-agent [-c] -i <input>\n";
    }
    $stream = build_csv_stream($input);
}

# Else if told that our command-line arguments are log archives, build a
# stream from the log files found there.
elsif ($archive) {
    if (!@ARGV) {
        die "Usage: webkdc-user-agent [-c] -a <directory> [...]\n";
    }
    $stream = build_archive_stream(@ARGV);
}

# Otherwise, expect log files on the command line and build a stream from them.
else {
    my ($error, $access) = @ARGV;
    if (!defined $access) {
        die "Usage: webkdc-user-agent [-c] <error> <access>\n";
    }
    $stream = build_log_stream([$error], [$access]);
}

# If told to just output CSV, print out each entry from our stream.
if ($csv) {
    my $printer = Text::CSV->new({ eol => "\n", quote_space => 0 });
    my @fields = qw(timestamp user user_agent);
    $printer->print(\*STDOUT, \@fields);
    while (my $entry = $stream->get) {
        my @data = @{$entry}{@fields};
        $printer->print(\*STDOUT, \@data);
    }
}

# Otherwise, generate user agent statistics.
else {
    my %agents;

    # Memoize simple_agent.  HTTP::BrowserDetect is slow.
    memoize('simple_agent');

    # For each user agent, store a hash of users who use that agent.
    while (my $entry = $stream->get) {
        my $agent = simple_agent($entry->{user_agent});
        my $user  = $entry->{user};
        $agents{$agent}{$user}++;
    }

    # Determine the maximum count of users for a given user agent.
    my @counts = map { scalar keys $agents{$_} } keys %agents;
    my $max_count = max @counts;

    # The width for printing will be the log base 10 of the max count.
    my $width = int(log($max_count) / log 10) + 1;

    # Determine the total number of unique users.  This isn't just the sum
    # of the counts, since some users will use multiple devices.
    my %users;
    for my $agent (keys %agents) {
        for my $user (keys %{ $agents{$agent} }) {
            $users{$user}++;
        }
    }
    my $user_count = scalar keys %users;

    # Sort the user agents by descending count of users and print totals.
    my $by_users = sub {
        scalar keys %{ $agents{$b} } <=> scalar keys %{ $agents{$a} };
    };
    for my $agent (sort $by_users keys %agents) {
        my $count = scalar keys $agents{$agent};
        printf {*STDOUT} "%${width}d %4.1f%% %s\n", $count,
          ($count / $user_count) * 100, $agent
          or die "Cannot print to standard output: $!\n";
    }
    say "\nTotal unique users: $user_count"
      or die "Cannot print to standard output: $!\n";
}

exit 0;
